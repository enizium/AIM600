{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4b-Thjw3HcxS"
   },
   "source": [
    "# Week 1 Lab- Sujan Maharjan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower casing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Read CSV file\n",
    "df = pd.read_csv('realdonaldtrump.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Be sure to tune in and watch Donald Trump on L...\n",
       "1    Donald Trump will be appearing on The View tom...\n",
       "2    Donald Trump reads Top Ten Financial Tips on L...\n",
       "3    New Blog Post: Celebrity Apprentice Finale and...\n",
       "4    \"My persona will never be that of a wallflower...\n",
       "5    Miss USA Tara Conner will not be fired - \"I've...\n",
       "6    Listen to an interview with Donald Trump discu...\n",
       "7    \"Strive for wholeness and keep your sense of w...\n",
       "8    Enter the \"Think Like A Champion\" signed book ...\n",
       "9    \"When the achiever achieves, it's not a platea...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print content data \n",
    "df['content'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    be sure to tune in and watch donald trump on l...\n",
       "1    donald trump will be appearing on the view tom...\n",
       "2    donald trump reads top ten financial tips on l...\n",
       "3    new blog post: celebrity apprentice finale and...\n",
       "4    \"my persona will never be that of a wallflower...\n",
       "5    miss usa tara conner will not be fired - \"i've...\n",
       "6    listen to an interview with donald trump discu...\n",
       "7    \"strive for wholeness and keep your sense of w...\n",
       "8    enter the \"think like a champion\" signed book ...\n",
       "9    \"when the achiever achieves, it's not a platea...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'content' column to lowercase\n",
    "df['content'] = df['content'].str.lower()\n",
    "df['content'].head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    be sure to tune in and watch donald trump on l...\n",
       "1    donald trump will be appearing on the view tom...\n",
       "2    donald trump reads top ten financial tips on l...\n",
       "3    new blog post celebrity apprentice finale and ...\n",
       "4    my persona will never be that of a wallflower ...\n",
       "5    miss usa tara conner will not be fired  ive al...\n",
       "6    listen to an interview with donald trump discu...\n",
       "7    strive for wholeness and keep your sense of wo...\n",
       "8    enter the think like a champion signed book an...\n",
       "9    when the achiever achieves its not a plateau i...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Define function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Apply function to 'Comments' column\n",
    "df['content'] = df['content'].apply(remove_punctuation)\n",
    "df['content'].head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords (run this line only once)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define function to remove stopwords\n",
    "stop_words = set(stopwords.words(' donald trump'))\n",
    "def remove_stopwords(text):\n",
    "    filtered_text = \" \".join(word for word in text.split() if word.lower() not in stop_words)\n",
    "    return filtered_text\n",
    "\n",
    "# Apply function to 'Comments' column\n",
    "df['content'] = df['content'].apply(remove_stopwords)\n",
    "\n",
    "df['content'].head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Text in Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize 'Text' column\n",
    "df['content'] = df['content'].apply(word_tokenize)\n",
    "\n",
    "df['content'].head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK Word Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define a function to apply stemming to a string\n",
    "def stem_text(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "# Apply stemming to the 'Text' column\n",
    "df['Stemmed Text'] = df['content'].apply(stem_text)\n",
    "\n",
    "df['Stemmed Text'].head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to lemmatize words\n",
    "def lemmatize_word(word):\n",
    "    # Find the part of speech for the word\n",
    "    pos = get_wordnet_pos(nltk.pos_tag([word])[0][1])\n",
    "    # Lemmatize the word based on its part of speech\n",
    "    if pos:\n",
    "        return lemmatizer.lemmatize(word, pos)\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word)\n",
    "\n",
    "# Define a function to get the part of speech for a word\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Lemmatize each word in the tweets\n",
    "df['lemmatized_text'] = df['content'].apply(lambda x: ' '.join([lemmatize_word(word) for word in x.split()]))\n",
    "\n",
    "df['lemmatized_text'].head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and Lemmatization Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to perform stemming on words\n",
    "def stem_word(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "# Define a function to perform lemmatization on words\n",
    "def lemmatize_word(word):\n",
    "    # Find the part of speech for the word\n",
    "    pos = get_wordnet_pos(nltk.pos_tag([word])[0][1])\n",
    "    # Lemmatize the word based on its part of speech\n",
    "    if pos:\n",
    "        return lemmatizer.lemmatize(word, pos)\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word)\n",
    "\n",
    "# Define a function to get the part of speech for a word\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Stem each word in the tweets\n",
    "df['stemmed_text'] = df['content'].apply(lambda x: ' '.join([stem_word(word) for word in x.split()]))\n",
    "\n",
    "# Lemmatize each word in the tweets\n",
    "df['lemmatized_text'] = df['content'].apply(lambda x: ' '.join([lemmatize_word(word) for word in x.split()]))\n",
    "\n",
    "# Compare stemmed and lemmatized text with original text\n",
    "for i in range(10):\n",
    "    print(f\"Original text: {df['text'][i]}\")\n",
    "    print(f\"Stemmed text: {df['stemmed_text'][i]}\")\n",
    "    print(f\"Lemmatized text: {df['lemmatized_text'][i]}\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Plotting Frequencies of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tokenize the tweets into words\n",
    "words = []\n",
    "for tweet in df['content']:\n",
    "    words.extend(word_tokenize(tweet.lower()))\n",
    "\n",
    "# Get the frequency distribution of words\n",
    "freq_dist = nltk.FreqDist(words)\n",
    "\n",
    "# Plot the frequency distribution of the top 20 words\n",
    "plt.figure(figsize=(10, 5))\n",
    "freq_dist.plot(20)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "plt.title('Frequency distribution of top 20 words')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
